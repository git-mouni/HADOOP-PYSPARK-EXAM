{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e54339c",
   "metadata": {},
   "source": [
    "### Big Data Hadoop & Spark Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a2f58",
   "metadata": {},
   "source": [
    "### Problem Statement:\n",
    "### Perform the following tasks using PySpark\n",
    "### Q.1 Read the given CSV file in a Hive table\n",
    "### Q2. Read the data from Hive table as spark dataframe\n",
    "### Q3. Get the correlation between dependent and independent variables\n",
    "### Q4. Build a linear regression model to predict house price\n",
    "### Q5. Evaluate the Linear Regression model by getting the RMSE and R-squared values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068f3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ea59b9a",
   "metadata": {},
   "source": [
    "### Ignoring Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe52dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6e94a",
   "metadata": {},
   "source": [
    "### Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e614e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc= SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349508a4",
   "metadata": {},
   "source": [
    "### Reading the dataset and converting it into a hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f681b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.00632, ZN=18.0, INDUS=2.309999943, CHAS=0, NOX=0.537999988, RM=6.574999809, AGE=65.19999695, DIS=4.090000153, RAD=1, TAX=296, PT=15.30000019, B=396.8999939, LSTAT=4.980000019, MV=24.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('boston.csv')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af326b0c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "axs = pd.plotting.scatter_matrix(sampled_data, figsize=(10, 10))\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982112e6",
   "metadata": {},
   "source": [
    "### Correlations between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c0f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to MV for  CRIM -0.3883046116575088\n",
      "Correlation to MV for  ZN 0.36044534463752903\n",
      "Correlation to MV for  INDUS -0.48372517128143383\n",
      "Correlation to MV for  CHAS 0.17526017775291847\n",
      "Correlation to MV for  NOX -0.4273207763683772\n",
      "Correlation to MV for  RM 0.695359937127267\n",
      "Correlation to MV for  AGE -0.37695456714288667\n",
      "Correlation to MV for  DIS 0.24992873873512172\n",
      "Correlation to MV for  RAD -0.3816262315669168\n",
      "Correlation to MV for  TAX -0.46853593528654536\n",
      "Correlation to MV for  PT -0.5077867038116085\n",
      "Correlation to MV for  B 0.3334608226834164\n",
      "Correlation to MV for  LSTAT -0.7376627294671615\n",
      "Correlation to MV for  MV 1.0\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "for i in df.columns:\n",
    "    if not( isinstance(df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to MV for \", i, df.stat.corr('MV',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832dae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|         MV|\n",
      "+--------------------+-----------+\n",
      "|[0.00632,18.0,2.3...|       24.0|\n",
      "|[0.027310001,0.0,...|21.60000038|\n",
      "|[0.02729,0.0,7.07...|34.70000076|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PT', 'B', 'LSTAT'], outputCol = 'features')\n",
    "vdf = vectorAssembler.transform(df)\n",
    "vdf = vdf.select(['features', 'MV'])\n",
    "vdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2202a55",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64aa70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vdf.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1cc1d",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee649ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.04448710389959217,0.009971542914437687,-0.0335038036498445,2.9165655866061764,-2.9443542801818454,3.6290521011439023,0.0,-0.714993622931146,0.026216352820383728,0.0,-0.7866654549852938,0.010919602503659176,-0.6181908570025751]\n",
      "Intercept: 22.593611583368922\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='MV', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c80069",
   "metadata": {},
   "source": [
    "### Evaluvation of the model using RMSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb20e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.881559\n",
      "r2: 0.712035\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243cfe3",
   "metadata": {},
   "source": [
    "### Predicting the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9fc5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|  31.2936891196482|       24.0|[0.00632,18.0,2.3...|\n",
      "|31.585057653339263|32.70000076|[0.01301,35.0,1.5...|\n",
      "|29.751673813545395|35.40000153|[0.01311,90.0,1.2...|\n",
      "| 30.59167105337913|29.10000038|[0.01439,60.0,2.9...|\n",
      "| 38.74669131239574|       50.0|[0.020090001,95.0...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.724054\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"MV\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e0602",
   "metadata": {},
   "source": [
    "### RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbe6482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.93718\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a040ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 10\n",
      "objectiveHistory: [0.5, 0.4354732271596991, 0.24732703438595374, 0.2239625484138751, 0.19292632169256632, 0.18800192683188915, 0.1866413088184445, 0.1851980568153613, 0.1846183570636647, 0.18398027249446722, 0.18334831937070936]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|  2.040931630960319|\n",
      "| -5.327131773349777|\n",
      "| 2.1012346165965923|\n",
      "| 12.081301831438381|\n",
      "| 0.3798948082596887|\n",
      "| -2.952884346324197|\n",
      "|  8.576031950676871|\n",
      "|  9.407891103839006|\n",
      "|  3.964619815953057|\n",
      "|  2.023764052809625|\n",
      "| -2.859448851911935|\n",
      "|  7.553404751823816|\n",
      "| 0.3459752069061217|\n",
      "|-1.3136132497002109|\n",
      "| 6.1572769419371625|\n",
      "|  3.354056880425915|\n",
      "| -3.858834526286202|\n",
      "| 1.4147156918201311|\n",
      "|-2.1967597888027264|\n",
      "|  0.873357539968449|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f09cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|         MV|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|  31.2936891196482|       24.0|[0.00632,18.0,2.3...|\n",
      "|31.585057653339263|32.70000076|[0.01301,35.0,1.5...|\n",
      "|29.751673813545395|35.40000153|[0.01311,90.0,1.2...|\n",
      "| 30.59167105337913|29.10000038|[0.01439,60.0,2.9...|\n",
      "| 38.74669131239574|       50.0|[0.020090001,95.0...|\n",
      "|31.082838331555124|31.10000038|[0.02187,60.0,2.9...|\n",
      "| 26.68630293182535|       16.5|[0.024979999,0.0,...|\n",
      "|27.972070950008696|23.89999962|[0.025429999,55.0...|\n",
      "| 26.62196482199333|28.70000076|[0.029850001,0.0,...|\n",
      "|20.304116117133706|       18.5|[0.030409999,0.0,...|\n",
      "|29.584750390794248|31.20000076|[0.03049,55.0,3.7...|\n",
      "| 30.09201779628995|33.40000153|[0.032370001,0.0,...|\n",
      "| 22.75497419359761|20.60000038|[0.033059999,0.0,...|\n",
      "|29.091127273078243|       23.5|[0.035840001,80.0...|\n",
      "|28.845342524393196|27.89999962|[0.036150001,80.0...|\n",
      "| 33.65174348996041|34.59999847|[0.03768,80.0,1.5...|\n",
      "|  26.0644839886203|23.20000076|[0.038710002,52.5...|\n",
      "| 35.32033978881063|33.29999924|[0.040109999,80.0...|\n",
      "| 31.40134766582027|30.29999924|[0.046659999,80.0...|\n",
      "| 27.00011866982294|22.60000038|[0.046840001,0.0,...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"MV\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394edaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
